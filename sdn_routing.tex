
In this work, I develop SDN-based routing techniques for both full-bisection fat-trees and tapered fat-trees, with support for single-path as well as multipath routing. My primary objective is to minimize congestion and balance link utilization—particularly for large elephant flows—in order to improve overall communication performance in high-performance computing systems.

Elephant flows are long-lived and bandwidth-intensive. When multiple such flows share the same network link, they contend for limited bandwidth and introduce significant congestion. This behavior makes elephant flows the primary contributors to network bottlenecks in HPC and datacenter environments. Accordingly, I define congestion as a measure of contention on a link, quantified by the number of active elephant flows traversing it.

Let the network be modeled as a directed graph \( G = (V, E) \), where \( V \) is the set of switches and terminals, and \( E \) is the set of directed links. Let \( F = \{f_1, f_2, \dots, f_n\} \) denote the set of elephant flows in the network. Each flow \( f \in F \) is assigned a single path \( P_f \subseteq E \) by the SDN controller.

For single-path routing, the congestion \( c(e) \) on a link \( e \in E \) is defined as:

\[
c(e) = \sum_{f \in F} \mathbf{1}_{\{e \in P_f\}}
\]

Here, \( \mathbf{1}_{\{e \in P_f\}} \) is an indicator function that equals 1 if link \( e \) is part of the path assigned to flow \( f \), and 0 otherwise.

The maximum link congestion across the network is then given by:

\[
C_{\text{max}} = \max_{e \in E} c(e)
\]

This congestion metric forms the basis for evaluating routing efficiency in my single-path SDN scheme. It guides the routing decision process by identifying and minimizing the worst-case contention experienced on any single link in the network.


\subsection{SDN-Singlepath Routing}

In single-path SDN routing, I assign each elephant flow a unique route through the network. Unlike traditional static routing methods, which rely on preconfigured tables, SDN enables dynamic path selection using a global view of current link utilization. This centralized perspective allows the controller to make informed decisions that minimize congestion and avoid oversubscription on critical links.

To explore the design space of single-path routing under SDN, I implement two complementary strategies: \textit{SDN-greedy} and \textit{SDN-optimal} which we talk about in the following sections.

\subsubsection{SDN-greedy}

\textit{SDN-greedy} is a lightweight, single-path routing algorithm designed to operate on both full-bisection and tapered fat-tree topologies. The algorithm leverages the SDN controller’s global view of the network to make path selection decisions that minimize link congestion in real time.

As shown in algorithm~\ref{alg:sdn_greedy}, the controller processes each elephant flow individually. For a given flow, it enumerates all feasible paths between the source and destination switches. For each candidate path, it computes the maximum link congestion \( c(e) \) across all links \( e \) on the path, which corresponds to the path's worst-case contention. This value is equivalent to evaluating the local \( C_{\text{max}} \) for that path. The controller then selects the path with the smallest such value and assigns it to the flow.

By repeating this process for all elephant flows in the current scheduling phase, the algorithm constructs a routing table that seeks to keep the network-wide maximum link congestion low. This greedy strategy effectively spreads traffic across the network to reduce the risk of bottlenecks. Due to its simplicity and responsiveness, SDN-greedy is well-suited for environments with light to moderate traffic.

However, because flows are routed independently, SDN-greedy may cause multiple flows to converge on the same links, especially under heavy or bursty traffic. This lack of global coordination can lead to suboptimal load balancing in dense communication scenarios.

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{SDN-greedy algorithm}
\label{alg:sdn_greedy}
\KwInput{Elephant flows in a phase $EF$}
\KwOutput{Load-balanced routing table $Routing\_table$}
\SetKwFunction{FMain}{ComputeLoadBalancedRoutingTable}
\SetKwProg{Fn}{Function}{:}{}
\Fn{\FMain{$EF$}}{
    Initialize an empty map $Routing\_table$\;
    \ForEach{$E \in EF$}{
        Get current link loads in the network\;
        Get all possible paths for $Source$ and $Destination$ of $E$\;
        \ForEach{$path$ in $All\_possible\_paths$}{
            Compute maximum link congestion $c(e)$ for links in $path$\;
        }
        Add path with minimum $C_{\text{max}}$ to $Routing\_table$ for $E$\;
    }
    \KwRet{$Routing\_table$}\;
}
\end{algorithm}



\subsection{SDN-Optimal Routing Algorithm}

In fat-tree topologies, particularly full-bisection fat-trees, minimizing link contention is crucial to improving communication performance. In this work, I introduce a single-path routing algorithm, called \textit{SDN-optimal}, that achieves the theoretically lowest possible value of maximum link congestion, denoted by \( C_{\text{max}} \), across all network links. Specifically, SDN-optimal guarantees that \( C_{\text{max}} = 1 \) for any permutation of elephant flows in a full-bisection fat-tree. This is achieved by (1) partitioning the flows into a minimum number of disjoint permutations, and (2) scheduling each permutation such that no two flows share any link. The second step leverages the structural properties of full-bisection fat-trees and Hall’s Marriage Theorem to ensure contention-free routing.

\subsubsection{SDN-optimal Algorithm for Full-Bisection Fat-Trees}

The SDN-optimal algorithm consists of two main steps: (1) partitioning the traffic pattern into \( k \) disjoint permutations, and (2) routing each permutation using a contention-free schedule.

\paragraph{Step 1: Flow Partitioning into Permutations.}  
Given a set of elephant flows \( F \), I construct a bipartite graph \( G = (S, D, E) \), where \( S \) and \( D \) are source and destination nodes, and each edge \( e \in E \) represents a flow. To ensure that each source and destination has the same number of flows (i.e., a regular graph), I add dummy nodes and edges to make the graph \( k \)-regular, where \( k \) is the maximum degree in the original graph. I then apply edge-coloring (via Kőnig’s Theorem) to decompose the graph into \( k \) disjoint perfect matchings. Each matching corresponds to a permutation \( P_1, P_2, ..., P_k \), where each node appears exactly once per permutation. The algorithm is described in algorithm ~\ref{alg:partition_permutations}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Flow Partitioning into Disjoint Permutations}
\label{alg:partition_permutations}

\KwInput{Flow set $F$, max degree $k$}
\KwOutput{$k$ disjoint permutations $P_1, ..., P_k$}

Construct bipartite graph $G = (S, D, E)$ from flows in $F$\;

Pad $G$ with dummy nodes/edges to make it $k$-regular\;

Apply edge-coloring to obtain $k$ disjoint matchings $P_1, ..., P_k$\;

Remove dummy flows from each $P_i$\;

\KwRet{$P_1, ..., P_k$}
\end{algorithm}

\paragraph{Step 2: Contention-Free Scheduling.}  
For each permutation \( P_i \), the flows are routed through the fat-tree using link-disjoint paths. In a full-bisection bandwidth of the topology, because the number of flows per permutation equals the number of available links at each network layer, Hall’s Marriage Theorem guarantees the existence of a perfect matching, no link will be used by more than one flow in a given permutation. Therefore, the maximum congestion per link in each permutation is exactly 1. The algorithm is described in algorithm ~\ref{alg:contention_free_scheduling}


\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Contention-Free Scheduling for Permutation $P_i$}
\label{alg:contention_free_scheduling}

\KwInput{Permutation $P_i$, topology $G$, link usage table $L$}
\KwOutput{Routing paths for all flows in $P_i$}

\tcp{Step 1: Schedule Intra-Pod Flows}
\ForEach{intra-pod flow $f \in P_i$}{
  \ForEach{uplink $u$ from source leaf to aggregation}{
    \If{$u$ is unused}{
      Find corresponding downlink $d$ to destination leaf\;
      \If{$d$ is unused}{
        Assign path: leaf $\rightarrow$ agg $\rightarrow$ leaf\;
        Mark $u$, $d$ as used; \textbf{break}\;
      }
    }
  }
  \If{no path assigned}{
    Apply non-blocking rearrangement to free a valid intra-pod path\;
  }
}

\tcp{Step 2: Schedule Inter-Pod Flows}
\ForEach{inter-pod flow $f \in P_i$}{
  \ForEach{uplink $u_1$ from source leaf to aggregation}{
    \If{$u_1$ is unused}{
      \ForEach{uplink $u_2$ from aggregation to core}{
        \If{$u_2$ is unused}{
          Find downlink $d_2$ from core to destination agg\;
          \If{$d_2$ is unused}{
            Find downlink $d_1$ to destination leaf\;
            \If{$d_1$ is unused}{
              Assign full path: leaf $\rightarrow$ agg $\rightarrow$ core $\rightarrow$ agg $\rightarrow$ leaf\;
              Mark all links as used; \textbf{break}\;
            }
          }
        }
      }
    }
  }
  \If{no path assigned}{
    Apply non-blocking rearrangement to free a valid inter-pod path\;
  }
}
\end{algorithm}


\paragraph{Proof Sketch of Optimality.}

In a full-bisection fat-tree, the number of aggregation switches is equal to the number of leaf switches. Each leaf switch has \( \frac{K}{2} \) uplinks to aggregation switches. The total number of links between the leaf and aggregation switches is \( N_{\text{leaf}} \times \frac{K}{2} \). Since each compute node connects to a downlink port on a leaf switch, the total number of compute nodes is also \( N_{\text{leaf}} \times \frac{K}{2} \). In a full permutation traffic pattern, each compute node sends and receives exactly one flow, so the total number of flows in each permutation is equal to the number of compute nodes. Therefore, the number of flows in each permutation is equal to the number of links between the leaf and aggregation switches.

Similarly, the number of links between the aggregation and core switches is also equal to the number of flows in each permutation, because each aggregation switch has \( \frac{K}{2} \) uplinks to core switches. As a result, the number of flows in each permutation exactly matches the number of available links at each layer of the network.

This one-to-one correspondence between flows and available links allows the use of Hall’s Marriage Theorem to guarantee a contention-free assignment. Suppose, for contradiction, that in some permutation, a link is used by more than one flow. Then at least one other link must remain unused, violating the condition required for a perfect matching. This contradiction implies that no two flows in the same permutation can share a link. Therefore, the maximum congestion on any link in any permutation is 1. Since this holds for every permutation, the maximum link congestion across all permutations is also 1. Thus, the SDN-optimal algorithm guarantees \( C_{\text{max}} = 1 \) and achieves contention-free routing in a full-bisection fat-tree.

\subsubsection{SDN-Optimal for Tapered Fat-Trees}

In a tapered fat-tree, bandwidth reduction occurs at higher layers, such as the aggregate to core level has less links than leaf to aggregate level. This architectural tapering leads to insufficient link capacity when routing permutation traffic, where the number of flows often exceeds the number of available links at higher levels. As a result, flows begin to contend for shared links, and the assumptions of perfect matching guaranteed by Hall’s Marriage Theorem no longer hold. Consequently, achieving a contention-free assignment for all flows within a single permutation becomes infeasible.

To overcome this limitation and preserve optimal scheduling, the SDN-optimal routing strategy partitions the full permutation into multiple sub-permutations. Each sub-permutation is constructed such that the number of flows passing through each layer of the network matches the number of available links at that layer. This ensures that within each sub-permutation, contention-free routing is still possible using the techniques previously employed.

Our goal is to make sure that we create sub-permuatations by selects flows in such a way that the links in between each fat-tree layers have no contention and has the maximum utilization. 
To construct these sub-permutations, the algorithm first selects flows that traverse the core layer—typically inter-pod flows that consume both leaf-to-aggregate and aggregate-to-core links. It continues selecting such flows until all available core-level links are utilized. At this point, adding more inter-pod flows would introduce contention. The algorithm then fills the remaining capacity at the aggregation layer by selecting intra-pod flows, which consume only leaf-to-aggregate and aggregate-to-leaf links. The result is a sub-permutation that fully utilizes available link resources without exceeding capacity at any layer. This allows Hall’s Marriage Theorem to be applied to guarantee a conflict-free routing for each sub-permutation.

\begin{algorithm}[H]
\caption{Sub-Permutation Construction Using Core and Aggregate Link Counters}
\label{alg:sub_permutation}
\KwIn{
    $F_{\text{core}}$: set of flows that traverse the core switch\\
    $F_{\text{agg}}$: set of flows that only traverse the aggregate switch\\
    $c$: number of available core-to-aggregate links per sub-permutation\\
    $a$: number of available aggregate-to-leaf links per sub-permutation
}
\KwOut{
    $P_{\text{list}}$: list of sub-permutations
}

$P_{\text{list}} \gets \emptyset$\;

\While{$F_{\text{core}} \neq \emptyset$ \textbf{or} $F_{\text{agg}} \neq \emptyset$}{
    $P \gets \emptyset$\;
    $core\_links \gets c$\;
    $agg\_links \gets a$\;

    \tcp{Stage 1: Add core-level flows}
    \ForEach{$f \in F_{\text{core}}$}{
        Add $f$ to $P$\;
        $core\_links \gets core\_links - 1$\;
        Remove $f$ from $F_{\text{core}}$\;
        \If{$core\_links == 0$ \textbf{or} $F_{\text{core}} == \emptyset$}{
            \textbf{break}
        }
    }

    \tcp{Stage 2: Add aggregate-level flows}
    \ForEach{$f \in F_{\text{agg}}$}{
        Add $f$ to $P$\;
        $agg\_links \gets agg\_links - 1$\;
        Remove $f$ from $F_{\text{agg}}$\;
        \If{$agg\_links == 0$ \textbf{or} $F_{\text{agg}} == \emptyset$}{
            \textbf{break}
        }
    }

    Append $P$ to $P_{\text{list}}$\;
}

\Return $P_{\text{list}}$\;
\end{algorithm}


\subsection{SDN-Multipath Routing}

Multipath routing enables traffic to be split across multiple paths, improving bandwidth utilization and often reducing congestion. However, in many scenarios where flow paths overlap significantly, for example in shift traffic, this can introduce additional contention. In contrast, single-path routing (such as SDN-optimal) can perform better when flows are carefully scheduled to avoid bottlenecks in those cases.

To intelligently decide between these two strategies, I propose \textit{SDN-adaptive} routing for the multipath routing.
\subsubsection{SDN-adaptive}
The key idea behind SDN-adaptive is to first gather runtime information about the application’s traffic characteristics. During an initial profiling phase, SDN-adaptive collects communication performance data by running one iteration using adaptive multipath routing. This information is then sent to the SDN controller, which uses it decide the routing.
The process begins by executing one iteration of the application using adaptive multipath routing. The SDN controller records the communication time from this iteration as a reference. Then, the full simulation is restarted using SDN-optimal routing. After completing the first iteration of this SDN-optimal run, the controller compares the current communication time with the previously recorded time from the adaptive run.

If SDN-optimal demonstrates better performance (i.e., lower communication time), it is retained for the rest of the simulation. If, however, SDN-optimal is slower than adaptive, this indicates that multipath routing is more effective for the application's communication pattern. In that case, the controller immediately switches to adaptive routing for the remainder of the simulation.

This design allows the routing policy to be tuned to the observed behavior of the application, ensuring better adaptability across diverse workloads. The algorithm is described in algorithm ~\ref{alg:sdn_adaptive_api}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{SDN-adaptive algorithm}
\label{alg:sdn_adaptive_api}

\KwInput{%
  Application $A$; \\
  Topology $G$; \\
  Routing schemes: SDN-optimal and Adaptive
}
\KwOutput{%
  Final routing strategy and flow configuration
}

Run one iteration of $A$ using Adaptive routing\;
Record $\mathrm{commTime}_{\text{Adaptive}}$\;

Start full simulation of $A$ using SDN-optimal routing\;
After first iteration, record $\mathrm{commTime}_{\text{SDN-optimal}}$\;

\If{$\mathrm{commTime}_{\text{SDN-optimal}} < \mathrm{commTime}_{\text{Adaptive}}$}{
  Continue simulation with SDN-optimal routing\;
}
\Else{
  Switch to Adaptive routing for remainder of simulation\;
}
\end{algorithm}

