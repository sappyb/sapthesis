\chapter{Introduction}

Large-scale systems aiming to achieve over 1 Exaflop/s of sustained performance
are currently under construction. Unlike the systems dominating the HPC industry
a decade ago, many of today's and future systems consist of a relatively modest
number of nodes. For instance, Sequoia at LLNL \cite{sequa}, the fastest supercomputer in
the Top500 list in 2012, utilized 96K nodes to achieve 20 Petaflop/s of peak
performance. In comparison, Summit at ORNL \cite{summit}, one of the fastest
supercomputers as of June 2020, employs approximately 4600 nodes but achieves a
peak performance of 200 Petaflop/s. The driving force behind the reduction in the
number of nodes is compute acceleration devices such as GPUs \cite{owens2008gpu}. For example,
an NVIDIA Volta V100 can perform 7 Teraflop/s worth of double-precision
computation compared to 200 Gigaflop/s for a Blue Gene/Q node. However, such a
significant increase in computing capability has not been matched by a similar
increase in network capability. Additionally, the communication performance
achievable by an applications on a system remains the major bottleneck for the overall 
application performance.  Therefore, while communication needs tend to expand at a slower
rate compared to computational demands (e.g., analogous to the growth of surface
area versus volume), it remains vital to determine the optimal utilization of
existing communication capabilities and achieve an ideal balance between
computation and communication capabilities.  The central inquiry we aim to
address is whether a system featuring fewer nodes, each possessing greater
computing capability, outperforms a system comprising more nodes, each with
lesser computing capability. 


Software Defined Networking (SDN) \cite{kreutz2014software} has emerged as a promising technology and
has been widely implemented across various network environments, including data
centers, campus networks, and wide-area networks. SDN offers several notable
features: (1) a centralized global network view for intelligent traffic and
resource management, (2) flexible per-flow management to accommodate varying
network traffic patterns, (3) network monitoring capabilities providing valuable
traffic statistics, and (4) ease of integrating new network functionalities and
services. These features empower SDN to effectively manage traffic at the flow
level using a centralized view and optimize network resource utilization for
improved performance compared to traditional networking infrastructures \cite{tr2016sdn}.

While these SDN capabilities hold appeal for High-Performance Computing (HPC)
systems and applications, SDN adoption within the HPC domain remains limited.
One reason for this is the absence of clear evidence demonstrating SDN's
superiority over existing networking technologies in high-end HPC systems that
employ sophisticated routing schemes. Existing SDN methodologies are primarily
tailored for internet and data-parallel applications like Hadoop and map-reduce
applications \cite{he2016firebird}, which have communication characteristics different from those
of HPC applications. Consequently, to achieve optimal performance on SDN-based
HPC systems, novel techniques that consider the unique communication patterns of HPC
applications are of utmost importance. 

\begin{comment}
Many HPC applications involve simulating physical
processes across numerous time steps, where each step entails similar tasks.
This results in phased behavior characterized by alternating computation and
communication phases often
repeating across different time steps and being static, as known to application
developers \cite{faraj2002communication} \cite{hong2013achieving}. Leveraging such static communication information can enhance
SDN's ability to support communications effectively and perform tasks
efficiently. Considering these characteristics, I develop SDN techniques
tailored for HPC systems. Flow identification plays a crucial role in this
process, as accurately classifying flows is essential for achieving high
performance in an SDN-capable network. 

The focus of this work lies particularly on
identifying elephant flows \cite{he2016firebird}, which are flows with substantial data volumes,
as they typically dominate communication time in HPC applications. Subsequently,
we devise SDN-enhanced routing functions to adeptly schedule these elephant
flows and allocate optimal network resources. Building upon the first part of the work done 
on optimal system configurations, we will evaluate these new algorithms across
various interconnect topologies.  Such evaluations hold significant implications
for scientific computing, enabling researchers to harness the full potential of
HPC resources for groundbreaking discoveries across various fields.
\end{comment}


Our research primarily focuses on the following:

\begin{itemize} 
\item Understanding the performance implication of technology shifts.
I am using end-to-end system simulations to explore the
performance impact of various network design and parameter choices for
GPU-based systems. Conducting a sensitivity study of the overall performance with respect to
 change in these parameters.  
\item Developing and evaluating SDN techniques for HPC systems/applications.
I am investigating the influence of SDN on HPC
environments, whether SDN can improve the communication performance for HPC
systems, and whether our proposed techniques result in higher communication
performance in SDN-capable interconnect topologies than existing schemes.
\end{itemize} 



The structure of this prospectus aligns with the outlined research objectives.
Chapter 2 will provide a background on interconnection technologies, 
SDN fundamentals and related works. In chapter 3, I will examine the
influence of network parameters on modern HPC systems. Chapter 4 will delve into
the implementation of SDN enhancements within HPC environments and evaluate
their impact on application performance. Finally, chapter 5 will offer
concluding remarks.

