\chapter{Design and Evauation of Techniques for HPC platforms with SDN-capable Interconnects} In
this study, I tackle the challenge of incorporating Software Defined
Networking (SDN) for optimally running High-Performance Computing (HPC)
applications. SDN is a new paradigm that is successful in other domains, but has not
been fully explored in HPC \cite{kreutz2014software, alalmaei2020sdn, he2016firebird}. 
SDN offers several advantages such as
\begin{itemize}

\item \textbf{Centralized Management :} A controller is responsible for managing the network resources.
\item \textbf{Global View of Network :} The SDN controller receives performance analytics from network resources to learn about the global state of the network.
\item \textbf{Traffic Optimization :} Enables dynamic routing of the traffic based on network condition.
\item \textbf{Programmability :} Programmable interfaces are available for customization of network services.

\end{itemize}
All the above advantages can be applied to the HPC environment. 
The goal of this research is to develop and
evaluate SDN techniques for HPC environments. I focus on these five areas:


\begin{comment}
This work attempts to accomplish the following objectives:
\begin{itemize}
\item To have an network API,
allowing users to indicate whether a communication is an elephant flow or not. This
would release the network system from classifying such flows.

\item To create routing algorithm which can be implemented inside a
SDN controller, these routing functions help to load balance the network traffic
by efficiently scheduling the bandwidth heavy elephant flows which are classfied by the API.


\item To run a thorough simulation based evaluation of the techinques developed
in different node to rank mapping configurations.
\end{itemize}

The following sections propose the details of flow identication, SDN-enabled flow scheduling, topology and job mappings in this project.
\end{comment}

\section{Flow Identification}

\textcolor{blue}{What is it?}

In the context of SDN, network traffic usually has two kinds of flows. Elephant flows 
carry large amounts of data from a source to a destination. These flows
constitute a bulk of the network traffic and are sensitive to network bandwidth.
Mice flows on the other hand, carry a small amount of data from source to
destination \cite{yang2020flow, afek2015sampling}, and they are latency sensitive. For
applications I use, any flow that carries more than 100KB data is an elephant flow, and any other is a
mice flow.


\textcolor{blue}{Why you do it?}

By identifying elephant flows, SDN controllers can allocate network resources more effectively. The controller, armed with knowledge about these bandwidth-intensive flows, can prioritize and allocate appropriate links to handle their traffic. This proactive approach helps prevent congestion and ensures that critical flows receive sufficient bandwidth, leading to improved overall network performance.

\textcolor{blue}{How to do it?}

If the user knows
both the mapping of the ranks to nodes in the HPC system during the running of the
application and also the data sent in the MPI calls, they can determine the
communication characteristics of the application in the actual system, and can
determine which flows are elephants and which are mice when a HPC application
starts running on a system. 
In most HPC applications like the ones in Stencil4D shown in Figure \ref{code.stencil}, SDN
users (HPC application developers, compiler, or communication library) have the
knowledge of communication happening inside the program during the compile time.
Additionally, these set of neighbours determined in the compile time usually 
do not change during the runtime, which gives the user the
ability to correctly predict the communication characteristics 
To identify an elephant flow, the information about the message size of the
flow is needed. Therefore, in this project application is run initially with a certain node to rank mapping, and the
amount of data sent in each flow is collected. This data is stored in a matrix and then a program is used to filter
flows which send more than a certain threshold, which in my case is 100KB of data across a interval of 0.3 seconds.  
These flows are classified as elephant flows. The API gurantees that the user has ground truth about
the classified flows which happen in a network when a specific set of HPC applications are run on that 
system.

\textcolor{blue}{Significance of results?}

HPC applications tackle massive computations and data transfers, needing fast, responsive network connections. When researchers pinpoint the types of network flows(such, elephant flows and mice flows) these applications generate and simulate how they behave; they can understand how different factors like network topology and routing methods affect performance. Armed with this knowledge, they can fine-tune network designs to better suit the unique needs of various workloads, paving the way for more efficient systems down the road.

\begin{comment}
 First, HPC applications are often
massively parallelized, with various programming paradigms and libraries such as
Message Passing Interface (MPI) \cite{forum1994mpi}. The huge amount of computation that is
required in the application is divided into smaller computations and runs on
multiple computing nodes. Second, many HPC applications are tightly coupled â€“
there are dependencies among the parallel processes and the processes need to
communicate with each other during execution. Since many HPC applications
simulate physical phenomena in many iterations, the applications exhibit phased
behavior during execution with alternating communication and computation phases.
Communications in different iterations are often the same or have similar
characteristics. One common communication pattern for HPC applications is
nearest-neighbor where all the ranks which are spatially near each other take
part in communication. 
The main objective of this project is to harness the power of SDN to run HPC applications efficiently.
For HPC applications like the ones in Stencil4D shown in Figure \ref{code.stencil}, SDN
users (HPC application developers, compiler, or communication library) have the
knowledge of whether a communication is an elephant flow or not. The code indicates
that each node sends eight communication messages to its neighbors.
Additionally, since the neighbors are determined only during the compile time
and the set of neighbours do not change during the runtime, the user has the
ability to correctly predict the communication characteristic. 
If the user knows
both the mapping of the ranks to nodes in the HPC system during the running of the
application and also the data sent in the MPI calls, they can determine the
communication characteristics of the application in the actual system, and can
determine which flows are elephants and which are mice when a HPC application
starts running on a system. 
Even for some communications where the
destination is unknown such as the Lagos shown in
Figure \ref{code.laghos}. The API can be used to indicate that the unknown destination flow is
an elephant flow as long as the user can determine that the message size of the
communication is sufficiently large.
To identify an elephant flow, the information about the message size of the
flow is needed. Therefore, in this project application is run initially with a certain node to rank mapping, and the
amount of data sent in each flow is collected. This data is stored in a matrix and then a program is used to filter
flows which send more than a certain threshold of data across the entire simulation. 
These entire program is packaged in an API which classifies the flows as elephant flows. 
This API, gurantees that the user has ground truth about
the communication which happens in a network when a specific set of HPC applications are run on that 
system.
\end{comment}

\begin{figure}[hbtp]
\caption{Stencil4d Code snippet}
\label{code.stencil}
\begin{lstlisting}[breaklines]
for (int i = 0; i < MAX_ITER; i++) {
        MPI_Isend(sendn, 100000000, MPI_CHAR, north, 9, MPI_COMM_WORLD, &reqs[0]
);
        ...
        MPI_Irecv(recvn, 100000000, MPI_CHAR, north, 9, MPI_COMM_WORLD, &reqs[8]
);
        ...
        MPI_Waitall(16, req status);
}
\end{lstlisting}
\end{figure}


\begin{comment}
\begin{figure}[hbtp]
\caption{Laghos Code snippet}
\label{code.laghos}
\begin{lstlisting}[breaklines]
LagrangianHydroOperator (...){
        ...
        ParMesh *pm = H1FESpace.GetParMesh();
        MPI_Allreduce(&loc_area, &glob_area, 1, MPI_DOUBLE, MPI_SUM, pm->GetComm
());
        ...
}
\end{lstlisting}
\end{figure}
\end{comment}

\section{SDN-enabled flow scheduling}
This research proposes to develop a routing algorithm which aims to load balance the network traffic using the 
global view of a network through the SDN controller.
The algorithm intially uses a greedy approach which calculates the maximum link load for all available
paths for each elephant flow, and then adds whichever path has the minimum of the maximum link load 
-that is adds the path which is least heavily loaded for routing the elephant flow. 
The subnet manager provided by the SDN accumulates information
about the network state and flows over the polling interval. If the SDN
scheduler has no prior knowledge about the network flows, it uses D-mod-k routing.
At each polling interval, the traffic classification provided by the user is forwarded to the SDN scheduler 
to obtain a load-balanced routing table for the subsequent interval. This load balance schedule is a single 
path schedule. In this work, I also propose developing a multipath routing, where the buffer load for k least loaded
path is analysed, and the one which has the lowest buffer load among the k paths is used to route our network flow at every 
polling interval in the SDN controller. Finally,
the project proposes to develop an optimal single path routing, which tries to find a non-blocking path for 
every elephant flow in a permutation in a full bisection bandwidth fat-tree. This path is calculated in the SDN controller and 
updated in every polling interval. The significance of this research lies in its development of routing algorithms tailored for Software-Defined Networking (SDN) controllers. By focusing on efficient traffic management, adaptability to changing network conditions, and optimization for high-demand scenarios, this work addresses critical challenges in network performance and scalability.


\section{Impact of topology and routing schemes}
\begin{comment}
The project tries to evaluate all the SDN enhanced routing schemes, two single path routing and one multipath
routing, with both the widely popular static routing scheme in fat-tree called D-mod-k and the adaptive 
routing which is used in fat-trees. All the routing is schemes are evaluted on two different size
topologies, one mid-sized topology of 512 compute nodes and large-sized topology of 1024 compute
nodes. The project evalutes the developed techniques across 5 different applications, which are random permutation,
near neighbour, nekbone, lammps and milc; discussed the applications in chapter 2. 
\end{comment}
Topology and routing schemes can significantly impact the effectiveness
of SDN for supporting HPC applications. I propose to study a number of
widely used interconnect configurations in the HPC environment: fat-tree
with deterministic routing, fat-tree with adaptive routing, and dragonfly
between the processes happening in a way which uses the network resources efficiently.
 SDN techniques will be developed and evaluated for
these configurations. 
The results will give insight about the potential of different network architectures in conjunction with SDN to optimize performance, scalability, and resource utilization in HPC environments. This understanding will not only inform network design decisions but also contribute to the development of tailored SDN solutions that effectively address the unique challenges posed by high-performance computing workloads.


\section{SDN-aware communication}
MPI collective communications, such as MPI All-to-all, rely on different algorithms like Recursive Doubling, Bruck's Algorithm, and Gather-Scatter Algorithm. Each of these algorithms alters the communication behavior of the application. Leveraging Software-Defined Networking (SDN), I aim to create an SDN-aware communication system. By utilizing network state information from SDN, I will design an algorithm that optimizes communication between processes, ensuring efficient use of network resources.

\section{SDN-aware job scheduling}
This work explores the influence of node-to-rank mapping in our HPC system applications, with particular emphasis on three methods: linear mapping, random mapping, and SDN-aware job scheduling. Linear mapping organizes job ranks on nodes sequentially, maintaining spatial proximity between ranks. However, due to node fragmentation in real systems, complete linear mapping is not always feasible, leading to a blend of linear and random mapping for job ranks. Consequently, it becomes necessary to assess the effects of random mapping, where job ranks are assigned to available nodes randomly. This random assignment increases spatial distance between job ranks, consequently intensifying network resource usage for communication. To facilitate this evaluation, my preferred mapping is interfaced with the simulator via an API compatible with the simulator's network model. Finally, I also develop a job mapping strategy based on the 
flow classification where I incorporated the network flow classification done by the API into elephant and mice flows to create a job placement such that heavily communicating flows get the maximum bandwidth possible. The significance of this work lies in its investigation of the impact of node-to-rank mapping strategies on the performance of HPC system applications. By examining three distinct methods - linear mapping, random mapping, and SDN-aware job scheduling - the research sheds light on how different mapping approaches influence spatial proximity between job ranks and subsequent network resource usage for communication.
